# Privacy-Preserving-Data-Transformation

Introduction:
With the advent of social networks and our daily moving online, vast amount of information are uploaded, collected, and shared daily online without the knowledge of data owners. The holder of the data, e.g., an internet service provider, usually wants to analyze them and make them public for research purposes, commercial, statistical, or other purposes. This obviously raises concerns about privacy issues for data contributors. We need to employ methods such that the data can be used successfully for the intended purpose, as not only are the data vulnerable to leakages, but also to malicious and adversary inference by other parties. Therefore,  privacy-protection methods should be employed that allow data collectors and owners to control the types of information that can be inferred from their data.

 Consider a scenario where mobile users upload their sensor readings to the cloud, which in turn trains a classifier that allows smartphones to identify their users from sensor readings in the background. This approach benefits from the huge storage and computation resources of the cloud. However, without proper processing the same data can be used to infer sensitive information about users, such as location and activities performed. This is especially dangerous when the information is considered private.

 Some features sent by the user to the cloud may produce a threat to privacy and could be used to extract sensitive and private personal information, such as age, gender, etc. A popular method named data perturbation introduces perturbation on the data provided by the user. The data have the same statistics as the true data and can be used for training the classifier without making the sensitive information available.

Random Projection is a method for data perturbation and also reducing the dimensionality of the data by more than 50%. Some papers treated this problem as a single classification task. They introduce a supervised version of Principal Component Analysis (PCA) called Discriminant Component Analysis (DCA) and the projection directions are affected by the within-class scatter matrix as in Linear Discriminant Analysis. RUCA can be considered as a mixture of DCA and MDR, and it can also be extended to privacy-sensitive classifications. Experimental results on Human Activity Recognition data set show that this methodology can provide better classification accuracies for the desired task while outperforming state-of-the-art privacy preserving data projection methods in terms of accuracies obtained from privacy-sensitive classifications.

Fundamentally, this problem can be viewed as a pair of classification tasks: one task is intended (for example, identification of the person’s gender) while the other task is undesirable (for example, identification of a person’s ID). This task is called sensitive task In this context, we seek for an optimal data transformation that will maximally hurt the performance of any classifier for the undesired task without hurting the performance for the intended task. The other task which is called insensitive task is the one that we want to have a high classification accuracy. So our classifier we design should not hurt this task.
The methods are tested using the publicly available Human Activity Recognition data-set and the simulation results are reported in final section.
